# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Model Configuration
OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_MODEL=gpt-4

# Anthropic API Key (for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Anthropic Model Configuration
ANTHROPIC_MODEL=claude-3-haiku-20240307
# ANTHROPIC_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_MODEL=claude-3-opus-20240229

# Google API Key (for Gemini models)
GOOGLE_API_KEY=your_google_api_key_here

# Google Model Configuration
GOOGLE_MODEL=gemini-pro
# GOOGLE_MODEL=gemini-1.5-pro-latest

# Qdrant Configuration (optional, defaults to localhost:6333)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# LLM Selection (options: openai, huggingface, local, anthropic, google, ollama)
LLM_PROVIDER=huggingface

# Hugging Face Configuration (for open-source models)
# For better Hindi/multilingual support, uncomment one of these:

# Multilingual models with good Hindi support
# HUGGINGFACE_MODEL=ai4bharat/indic-bert
# HUGGINGFACE_MODEL=bert-base-multilingual-cased
# HUGGINGFACE_MODEL=facebook/xglm-4.5B
# HUGGINGFACE_MODEL=cross-encoder/stsb-xlm-roberta-base

# GLM models for multilingual support
# HUGGINGFACE_MODEL=THUDM/chatglm3-6b

# More powerful generation models
# HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
# HUGGINGFACE_MODEL=unsloth/Llama-3.2-3B-Instruct
# HUGGINGFACE_MODEL=google/mt5-xxl
# HUGGINGFACE_MODEL=bigscience/bloom-7b1

# Default (smaller model)
HUGGINGFACE_MODEL=THUDM/chatglm3-6b

# Local Model Configuration (for local models)
# For more powerful local models, uncomment one of these:

# Llama 3 models (very powerful)
# LOCAL_MODEL_PATH=TheBloke/Llama-3-8B-Instruct-GGUF/llama3-8b-instruct.Q4_K_M.gguf
# LOCAL_MODEL_PATH=TheBloke/Llama-3-70B-Instruct-GGUF/llama3-70b-instruct.Q4_K_M.gguf
# LOCAL_MODEL_TYPE=llama

# Mixtral (powerful MoE model)
# LOCAL_MODEL_PATH=TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
# LOCAL_MODEL_TYPE=mixtral

# Mistral models (good balance of power and efficiency)
LOCAL_MODEL_PATH=TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf
LOCAL_MODEL_TYPE=mistral

# Alternative paths
# LOCAL_MODEL_PATH=TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf

# Ollama Configuration
# For more powerful models with Ollama, uncomment one of these:

# Llama 3 models (very powerful)
# OLLAMA_MODEL=llama3:70b
# OLLAMA_MODEL=llama3:8b

# Mixtral (powerful MoE model)
# OLLAMA_MODEL=mixtral:8x7b
# OLLAMA_MODEL=mixtral:8x22b

# Mistral models
# OLLAMA_MODEL=mistral:7b
# OLLAMA_MODEL=mistral-large

# Gemma models (efficient)
# OLLAMA_MODEL=gemma2:27b
# OLLAMA_MODEL=gemma2:9b

# Default
OLLAMA_MODEL=llama3:8b

OLLAMA_BASE_URL=http://localhost:11434

# General Configuration
TEMPERATURE=0.7
MAX_NEW_TOKENS=1024
REPETITION_PENALTY=1.1
CONTEXT_LENGTH=8192

# Advanced Generation Parameters (for more powerful models)
TOP_P=0.9
TOP_K=50
BATCH_SIZE=512
GPU_LAYERS=0  # Set to >0 to enable GPU acceleration for local models
N_THREADS=0  # Use 0 to use all available CPU cores
SEED=-1